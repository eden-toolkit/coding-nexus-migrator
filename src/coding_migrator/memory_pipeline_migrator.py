"""
å†…å­˜æµæµæ°´çº¿è¿ç§»å™¨ - é›¶ç£ç›˜å ç”¨çš„è¾¹ä¸‹è½½è¾¹ä¸Šä¼ è¿ç§»
"""

import os
import json
import hashlib
import logging
import threading
import tempfile
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue, Empty
from dataclasses import dataclass, field
from tqdm import tqdm
import time
import io

from .models import MavenArtifact, MigrationConfig
from .coding_client import CodingClient
from .nexus_uploader import NexusUploader


logger = logging.getLogger(__name__)


@dataclass
class MemoryMigrationTask:
    """å†…å­˜è¿ç§»ä»»åŠ¡"""
    artifact: MavenArtifact
    file_hash: Optional[str] = None
    download_success: bool = False
    upload_success: bool = False
    error_message: Optional[str] = None
    file_data: Optional[bytes] = None
    created_time: float = field(default_factory=time.time)  # ä»»åŠ¡åˆ›å»ºæ—¶é—´


class MemoryPipelineMigrator:
    """å†…å­˜æµæµæ°´çº¿è¿ç§»å™¨ - é›¶ç£ç›˜å ç”¨"""

    def __init__(self, config: MigrationConfig):
        """
        åˆå§‹åŒ–å†…å­˜æµæ°´çº¿è¿ç§»å™¨

        Args:
            config: è¿ç§»é…ç½®
        """
        self.config = config
        self.coding_client = CodingClient(
            config.coding_token,
            config.coding_team_id,
            config.maven_repositories,
            config.pagination,
            config.performance.max_workers,
            requests_per_second=config.rate_limit.requests_per_second
        )
        self.nexus_uploader = NexusUploader(config)

        # æ€§èƒ½é…ç½®
        self.download_workers = config.performance.max_workers
        self.upload_workers = min(config.performance.max_workers // 2, 5)

        # ä»»åŠ¡é˜Ÿåˆ—
        self.upload_queue = Queue(maxsize=50)  # å‡å°é˜Ÿåˆ—å¤§å°ï¼Œå‡å°‘å†…å­˜å ç”¨
        self.completed_tasks = []
        self.failed_tasks = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_artifacts': 0,
            'downloaded': 0,
            'uploaded': 0,
            'download_failed': 0,
            'upload_failed': 0,
            'skipped_existing': 0
        }

        # æ§åˆ¶æ ‡å¿—
        self.stop_event = threading.Event()

        # è¿ç§»è®°å½•ç›®å½•ï¼ˆå›ºå®šç›®å½•ï¼Œæ”¯æŒå¢é‡è¿ç§»ï¼‰
        self.records_dir = Path("target")
        self.records_dir.mkdir(exist_ok=True)

        # å†…å­˜ç›‘æ§
        self.last_memory_check = time.time()
        self.memory_check_interval = 30  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡å†…å­˜

        # è¿ç§»è®°å½•æ–‡ä»¶å’Œä¾èµ–åˆ—è¡¨ï¼ˆå°†åœ¨migrate_projectä¸­åˆå§‹åŒ–ï¼‰
        self.record_file = None
        self.uploaded_hashes: Set[str] = set()
        self.uploaded_dependencies = []

        # å†…å­˜é™åˆ¶ï¼ˆå•ä½ï¼šå­—èŠ‚ï¼‰
        self.max_memory_usage = 100 * 1024 * 1024  # 100MB
        self.current_memory_usage = 0
        self.memory_lock = threading.Lock()

        # POM æ–‡ä»¶è¿½è¸ªå’Œè°ƒè¯•
        self.pom_stats = {
            'discovered_in_coding': 0,      # åœ¨CODINGä¸­å‘ç°çš„POMæ–‡ä»¶æ•°
            'skipped_already_uploaded': 0,  # è·³è¿‡å·²ä¸Šä¼ çš„POMæ–‡ä»¶æ•°
            'download_attempted': 0,        # å°è¯•ä¸‹è½½çš„POMæ–‡ä»¶æ•°
            'download_success': 0,          # ä¸‹è½½æˆåŠŸçš„POMæ–‡ä»¶æ•°
            'download_failed': 0,           # ä¸‹è½½å¤±è´¥çš„POMæ–‡ä»¶æ•°
            'upload_attempted': 0,          # å°è¯•ä¸Šä¼ çš„POMæ–‡ä»¶æ•°
            'upload_success': 0,            # ä¸Šä¼ æˆåŠŸçš„POMæ–‡ä»¶æ•°
            'upload_failed': 0,             # ä¸Šä¼ å¤±è´¥çš„POMæ–‡ä»¶æ•°
            'missing_in_nexus': []          # æœ€ç»ˆç¼ºå¤±çš„POMæ–‡ä»¶åˆ—è¡¨
        }
        self.pom_lock = threading.Lock()

        # å¤±è´¥ä¾èµ–è·¯å¾„æ—¥å¿—è®°å½•
        self.failed_logs_lock = threading.Lock()
        self.failed_download_log = self.records_dir / "failed_downloads.log"
        self.failed_upload_log = self.records_dir / "failed_uploads.log"

        # åˆå§‹åŒ–å¤±è´¥æ—¥å¿—æ–‡ä»¶
        self._init_failed_logs()

    def _init_failed_logs(self) -> None:
        """åˆå§‹åŒ–å¤±è´¥æ—¥å¿—æ–‡ä»¶"""
        try:
            # åˆ›å»ºä¸‹è½½å¤±è´¥æ—¥å¿—æ–‡ä»¶
            if not self.failed_download_log.exists():
                with open(self.failed_download_log, 'w', encoding='utf-8') as f:
                    f.write("# CODING Maven åˆ¶å“ä¸‹è½½å¤±è´¥è®°å½•\n")
                    f.write("# æ ¼å¼: æ—¶é—´æˆ³ | Mavenåæ ‡ | æ–‡ä»¶è·¯å¾„ | é”™è¯¯ä¿¡æ¯\n")
                    f.write("# Generated by Memory Pipeline Migrator\n\n")

            # åˆ›å»ºä¸Šä¼ å¤±è´¥æ—¥å¿—æ–‡ä»¶
            if not self.failed_upload_log.exists():
                with open(self.failed_upload_log, 'w', encoding='utf-8') as f:
                    f.write("# Nexus Maven åˆ¶å“ä¸Šä¼ å¤±è´¥è®°å½•\n")
                    f.write("# æ ¼å¼: æ—¶é—´æˆ³ | Mavenåæ ‡ | Mavenè·¯å¾„ | é”™è¯¯ä¿¡æ¯\n")
                    f.write("# Generated by Memory Pipeline Migrator\n\n")

            logger.debug(f"Failed logs initialized: {self.failed_download_log}, {self.failed_upload_log}")
        except Exception as e:
            logger.error(f"Failed to initialize failed logs: {e}")

    def _log_failed_download(self, artifact: MavenArtifact, error_message: str) -> None:
        """è®°å½•ä¸‹è½½å¤±è´¥çš„ä¾èµ–è·¯å¾„"""
        try:
            with self.failed_logs_lock:
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                maven_coord = f"{artifact.group_id}:{artifact.artifact_id}:{artifact.version}:{artifact.packaging}"

                log_entry = f"{timestamp} | {maven_coord} | {artifact.file_path} | {error_message}\n"

                with open(self.failed_download_log, 'a', encoding='utf-8') as f:
                    f.write(log_entry)

                logger.debug(f"Logged failed download: {maven_coord}")
        except Exception as e:
            logger.error(f"Failed to log download failure: {e}")

    def _log_failed_upload(self, artifact: MavenArtifact, maven_path: str, error_message: str) -> None:
        """è®°å½•ä¸Šä¼ å¤±è´¥çš„ä¾èµ–è·¯å¾„"""
        try:
            with self.failed_logs_lock:
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                maven_coord = f"{artifact.group_id}:{artifact.artifact_id}:{artifact.version}:{artifact.packaging}"

                log_entry = f"{timestamp} | {maven_coord} | {maven_path} | {error_message}\n"

                with open(self.failed_upload_log, 'a', encoding='utf-8') as f:
                    f.write(log_entry)

                logger.debug(f"Logged failed upload: {maven_coord}")
        except Exception as e:
            logger.error(f"Failed to log upload failure: {e}")

    def _load_migration_records(self) -> None:
        """åŠ è½½å·²è¿ç§»è®°å½•"""
        try:
            if self.record_file.exists():
                with open(self.record_file, 'r', encoding='utf-8') as f:
                    records = json.load(f)
                    self.uploaded_hashes = set(records.get('uploaded_hashes', []))
                    self.uploaded_dependencies = records.get('uploaded_dependencies', [])
                logger.info(f"Loaded {len(self.uploaded_hashes)} migration records")
                if self.uploaded_dependencies:
                    logger.info(f"Previously uploaded {len(self.uploaded_dependencies)} dependencies")
        except Exception as e:
            logger.warning(f"Failed to load migration records: {e}")
            self.uploaded_hashes = set()
            self.uploaded_dependencies = []

    def _save_migration_records(self) -> None:
        """ä¿å­˜è¿ç§»è®°å½•"""
        try:
            records = {
                'uploaded_hashes': list(self.uploaded_hashes),
                'uploaded_dependencies': self.uploaded_dependencies,
                'last_updated': time.time()
            }
            with open(self.record_file, 'w', encoding='utf-8') as f:
                json.dump(records, f, indent=2, ensure_ascii=False)
            logger.debug(f"Saved {len(self.uploaded_hashes)} migration records")
        except Exception as e:
            logger.error(f"Failed to save migration records: {e}")

    def _calculate_file_hash(self, file_data: bytes) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        return hashlib.sha256(file_data).hexdigest()

    def _check_if_already_uploaded(self, artifact: MavenArtifact) -> Optional[str]:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»ä¸Šä¼ è¿‡"""
        # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
        identifier = f"{artifact.group_id}:{artifact.artifact_id}:{artifact.version}:{artifact.packaging}"
        hash_identifier = hashlib.md5(identifier.encode()).hexdigest()

        if hash_identifier in self.uploaded_hashes:
            return hash_identifier
        return None

    def migrate_project(self, project_id: int, project_name: str) -> Dict[str, Any]:
        """
        è¿ç§»å•ä¸ªé¡¹ç›®ï¼ˆå†…å­˜æµæ¨¡å¼ï¼‰

        Args:
            project_id: é¡¹ç›® ID
            project_name: é¡¹ç›®åç§°

        Returns:
            è¿ç§»ç»“æœç»Ÿè®¡
        """
        logger.info(f"Starting memory pipeline migration for project: {project_name}")

        try:
            # åˆå§‹åŒ–è¿ç§»è®°å½•æ–‡ä»¶ï¼ˆæŒ‰é¡¹ç›®å’Œé¡¹ç›®IDåŒºåˆ†ï¼‰
            self.record_file = self.records_dir / f"migration_records_{project_name}_{project_id}.json"
            self._load_migration_records()

            # è·å–æ‰€æœ‰åˆ¶å“
            all_artifacts = self._get_all_artifacts(project_id)
            self.stats['total_artifacts'] = len(all_artifacts)

            if not all_artifacts:
                logger.warning(f"No artifacts found for project: {project_name}")
                return self.stats

            # è¿‡æ»¤å·²ä¸Šä¼ çš„åˆ¶å“
            filtered_artifacts = []
            for artifact in all_artifacts:
                existing_hash = self._check_if_already_uploaded(artifact)
                if existing_hash:
                    self.stats['skipped_existing'] += 1
                    logger.debug(f"Skipping already uploaded: {artifact.file_path}")
                else:
                    filtered_artifacts.append(artifact)

            logger.info(f"Found {len(all_artifacts)} total artifacts, {len(filtered_artifacts)} to migrate "
                       f"({self.stats['skipped_existing']} already uploaded)")

            if not filtered_artifacts:
                logger.info("All artifacts have already been migrated")
                return self.stats

            # å¯åŠ¨ä¸‹è½½å’Œä¸Šä¼ çº¿ç¨‹æ± 
            with ThreadPoolExecutor(max_workers=self.download_workers + self.upload_workers) as executor:
                # å¯åŠ¨ä¸Šä¼ å·¥ä½œçº¿ç¨‹
                upload_futures = []
                for i in range(self.upload_workers):
                    future = executor.submit(self._upload_worker)
                    upload_futures.append(future)

                # å¯åŠ¨ä¸‹è½½ä»»åŠ¡
                download_futures = []
                progress_bar = tqdm(total=len(filtered_artifacts), desc="Memory Pipeline",
                                  unit="files", postfix={"down": 0, "up": 0, "skip": self.stats['skipped_existing']})

                for artifact in filtered_artifacts:
                    future = executor.submit(self._download_and_queue, artifact, progress_bar)
                    download_futures.append(future)

                # ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ
                for future in as_completed(download_futures):
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"Download task failed: {e}")

                # ç­‰å¾…ä¸Šä¼ é˜Ÿåˆ—æ¸…ç©º
                logger.info("Waiting for upload queue to empty...")
                queue_wait_count = 0
                max_queue_wait = 600  # æœ€å¤§ç­‰å¾…60ç§’
                while not self.upload_queue.empty() and queue_wait_count < max_queue_wait:
                    queue_wait_count += 1
                    if queue_wait_count % 50 == 0:  # æ¯5ç§’æ‰“å°ä¸€æ¬¡
                        logger.info(f"Upload queue size: {self.upload_queue.qsize()}, waiting... ({queue_wait_count * 0.1:.1f}s)")
                    time.sleep(0.1)

                if not self.upload_queue.empty():
                    logger.warning(f"Upload queue not empty after timeout, {self.upload_queue.qsize()} items remaining")

                # åœæ­¢ä¸Šä¼ å·¥ä½œçº¿ç¨‹
                self.stop_event.set()

                # ç­‰å¾…ä¸Šä¼ çº¿ç¨‹å®Œæˆ
                for future in upload_futures:
                    future.result()

                progress_bar.close()

        except Exception as e:
            logger.error(f"Memory pipeline migration failed: {e}")
        finally:
            # ä¿å­˜è¿ç§»è®°å½•
            self._save_migration_records()

        # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡
        self._generate_final_stats()

        # æ˜¾ç¤ºå·²ä¸Šä¼ çš„ä¾èµ–æ±‡æ€»
        self._display_uploaded_dependencies_summary()

        # æ˜¾ç¤ºPOMæ–‡ä»¶è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
        self._display_pom_detailed_report()

        return self.stats

    def _generate_final_stats(self) -> None:
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        logger.info("=" * 60)
        logger.info("MEMORY PIPELINE MIGRATION SUMMARY")
        logger.info("=" * 60)
        logger.info(f"[OK] Total artifacts processed: {self.stats['total_artifacts']}")
        logger.info(f"â¬‡ï¸  Downloaded: {self.stats['downloaded']}")
        logger.info(f"â¬†ï¸  Uploaded: {self.stats['uploaded']}")
        logger.info(f"â­ï¸  Skipped (already uploaded): {self.stats['skipped_existing']}")
        logger.info(f"[ERROR] Download failed: {self.stats['download_failed']}")
        logger.info(f"[ERROR] Upload failed: {self.stats['upload_failed']}")
        logger.info("=" * 60)

    def _display_uploaded_dependencies_summary(self) -> None:
        """æ˜¾ç¤ºå·²ä¸Šä¼ ä¾èµ–çš„æ±‡æ€»ä¿¡æ¯"""
        if not self.uploaded_dependencies:
            logger.info("[INFO] No new dependencies uploaded in this session")
            return

        logger.info("")
        logger.info("[INFO] UPLOADED DEPENDENCIES SUMMARY")
        logger.info("=" * 60)

        # æŒ‰group_idå’Œartifact_idåˆ†ç»„æ˜¾ç¤º
        grouped_deps = {}
        for dep in self.uploaded_dependencies:
            key = f"{dep['group_id']}:{dep['artifact_id']}"
            if key not in grouped_deps:
                grouped_deps[key] = []
            grouped_deps[key].append(dep)

        for group_key, deps in sorted(grouped_deps.items()):
            logger.info(f"ğŸ“‹ {group_key}")
            for dep in sorted(deps, key=lambda x: x['version']):
                logger.info(f"   ğŸ·ï¸  {dep['version']} ({dep['packaging']}) - {dep['repository']}")

        logger.info("=" * 60)
        logger.info(f"ğŸ“ˆ Total dependencies uploaded: {len(self.uploaded_dependencies)}")
        logger.info("")

    def _get_all_artifacts(self, project_id: int) -> List[MavenArtifact]:
        """è·å–æ‰€æœ‰åˆ¶å“"""
        all_artifacts = []

        # è·å–é¡¹ç›®åç§°
        project_name = self.coding_client.get_project_name_by_id(project_id)

        # è·å–åˆ¶å“ä»“åº“
        repositories = self.coding_client.get_artifact_repositories(project_id)
        maven_repos = [repo for repo in repositories if repo.get('Type') == 3]

        for repo in maven_repos:
            repo_name = repo.get('Name', '')
            logger.info(f"Processing repository: {repo_name}")

            try:
                artifacts = self.coding_client.get_maven_artifacts(
                    project_id, repo_name, self.config.maven_filter
                )
                all_artifacts.extend(artifacts)
                logger.info(f"Found {len(artifacts)} artifacts in repository: {repo_name}")

                # ç»Ÿè®¡POMæ–‡ä»¶
                pom_artifacts = [art for art in artifacts if art.file_path.endswith('.pom')]
                with self.pom_lock:
                    self.pom_stats['discovered_in_coding'] += len(pom_artifacts)

                if pom_artifacts:
                    logger.info(f"[POM DEBUG] Found {len(pom_artifacts)} POM files in repository {repo_name}")
                    # è®°å½•å‰å‡ ä¸ªPOMæ–‡ä»¶ç”¨äºè°ƒè¯•
                    for i, pom in enumerate(pom_artifacts[:3]):
                        logger.info(f"[POM DEBUG]   POM {i+1}: {pom.group_id}:{pom.artifact_id}:{pom.version} ({pom.file_path})")
                    if len(pom_artifacts) > 3:
                        logger.info(f"[POM DEBUG]   ... and {len(pom_artifacts) - 3} more POM files")

            except Exception as e:
                logger.error(f"Failed to get artifacts from repository {repo_name}: {e}")

        return all_artifacts

    def _download_and_queue(self, artifact: MavenArtifact, progress_bar: tqdm) -> bool:
        """
        ä¸‹è½½åˆ¶å“åˆ°å†…å­˜å¹¶åŠ å…¥ä¸Šä¼ é˜Ÿåˆ—

        Args:
            artifact: åˆ¶å“ä¿¡æ¯
            progress_bar: è¿›åº¦æ¡

        Returns:
            ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        if self.stop_event.is_set():
            return False

        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸Šä¼ è¿‡
        existing_hash = self._check_if_already_uploaded(artifact)
        if existing_hash:
            # POMæ–‡ä»¶ç‰¹æ®Šå¤„ç†
            if artifact.file_path.endswith('.pom') or artifact.packaging == 'pom':
                with self.pom_lock:
                    self.pom_stats['skipped_already_uploaded'] += 1
                logger.info(f"[POM DEBUG] â­ï¸  SKIP POM: {artifact.group_id}:{artifact.artifact_id}:{artifact.version} already uploaded")
            else:
                logger.info(f"â­ï¸  SKIP: {artifact.group_id}:{artifact.artifact_id}:{artifact.version} already uploaded")
            self.stats['skipped_existing'] += 1
            progress_bar.set_postfix({"down": self.stats['downloaded'],
                                       "up": self.stats['uploaded'],
                                       "skip": self.stats['skipped_existing']})
            return True

        task = MemoryMigrationTask(artifact=artifact)

        try:
            # POMæ–‡ä»¶ä¸‹è½½ç»Ÿè®¡
            is_pom_file = artifact.file_path.endswith('.pom') or artifact.packaging == 'pom'
            if is_pom_file:
                with self.pom_lock:
                    self.pom_stats['download_attempted'] += 1
                logger.info(f"[POM DEBUG] ğŸ”„ STARTING POM DOWNLOAD: {artifact.group_id}:{artifact.artifact_id}:{artifact.version}")
                logger.info(f"[POM DEBUG]   File path: {artifact.file_path}")
                logger.info(f"[POM DEBUG]   Repository: {artifact.repository}")
                if hasattr(artifact, 'download_url') and artifact.download_url:
                    logger.info(f"[POM DEBUG]   Download URL: {artifact.download_url}")

            # å®šæœŸå†…å­˜æ£€æŸ¥å’Œæ¸…ç†
            current_time = time.time()
            if current_time - self.last_memory_check > self.memory_check_interval:
                self._check_and_cleanup_memory()
                self.last_memory_check = current_time

            # æ£€æŸ¥å†…å­˜ä½¿ç”¨é™åˆ¶
            with self.memory_lock:
                if self.current_memory_usage > self.max_memory_usage:
                    logger.warning(f"Memory usage limit reached: {self.current_memory_usage}/{self.max_memory_usage} bytes, waiting...")
                    wait_count = 0
                    max_wait_time = 300  # æœ€å¤§ç­‰å¾…30ç§’
                    while self.current_memory_usage > self.max_memory_usage * 0.5:
                        wait_count += 1
                        if wait_count % 10 == 0:  # æ¯1ç§’æ‰“å°ä¸€æ¬¡
                            logger.info(f"Waiting for memory release: {self.current_memory_usage}/{self.max_memory_usage} bytes (waited {wait_count * 0.1:.1f}s)")
                        if wait_count >= max_wait_time:  # è¶…æ—¶ä¿æŠ¤
                            logger.error(f"Memory wait timeout after {max_wait_time * 0.1:.1f}s, forcing continue")
                            # å¼ºåˆ¶æ¸…ç†ä¸€äº›å†…å­˜ï¼šæ¸…ç†å¤±è´¥çš„ä»»åŠ¡
                            self._emergency_memory_cleanup()
                            break
                        time.sleep(0.1)

            # ä¸‹è½½æ–‡ä»¶åˆ°å†…å­˜
            file_data = self._download_to_memory(artifact)

            if file_data:
                task.file_data = file_data
                task.file_hash = self._calculate_file_hash(file_data)
                task.download_success = True
                self.stats['downloaded'] += 1

                # POMæ–‡ä»¶ä¸‹è½½æˆåŠŸç»Ÿè®¡
                if is_pom_file:
                    with self.pom_lock:
                        self.pom_stats['download_success'] += 1
                    logger.info(f"[POM DEBUG] âœ… POM DOWNLOAD SUCCESS: {artifact.group_id}:{artifact.artifact_id}:{artifact.version}")
                    logger.info(f"[POM DEBUG]   File size: {len(file_data)} bytes")
                    logger.info(f"[POM DEBUG]   File hash: {task.file_hash[:16]}...")

                # æ›´æ–°å†…å­˜ä½¿ç”¨é‡
                with self.memory_lock:
                    self.current_memory_usage += len(file_data)

                # åŠ å…¥ä¸Šä¼ é˜Ÿåˆ—
                try:
                    self.upload_queue.put(task, timeout=30)
                except Exception as queue_error:
                    logger.error(f"Failed to add task to upload queue: {queue_error}")
                    task.error_message = f"Queue error: {queue_error}"
                    self.stats['upload_failed'] += 1
                    self.failed_tasks.append(task)

                    # è®°å½•ä¸Šä¼ å¤±è´¥åˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆé˜Ÿåˆ—é”™è¯¯ä¹Ÿç®—ä¸Šä¼ å¤±è´¥ï¼‰
                    maven_path = self._convert_to_maven_path(artifact)
                    self._log_failed_upload(artifact, maven_path, task.error_message)

                    # POMæ–‡ä»¶é˜Ÿåˆ—å¤±è´¥ç»Ÿè®¡
                    if is_pom_file:
                        with self.pom_lock:
                            self.pom_stats['upload_failed'] += 1
                        logger.error(f"[POM DEBUG] âŒ POM QUEUE FAILED: {artifact.group_id}:{artifact.artifact_id}:{artifact.version} - {queue_error}")
                    # é‡Šæ”¾å†…å­˜
                    with self.memory_lock:
                        self.current_memory_usage -= len(file_data)
                    logger.debug(f"Released memory due to queue error: {len(file_data)} bytes, current usage: {self.current_memory_usage}")
                    task.file_data = None
                    return False

                logger.debug(f"Downloaded and queued: {artifact.file_path} ({len(file_data)} bytes)")

            else:
                task.error_message = "Download failed"
                self.stats['download_failed'] += 1
                self.failed_tasks.append(task)

                # è®°å½•ä¸‹è½½å¤±è´¥åˆ°æ—¥å¿—æ–‡ä»¶
                self._log_failed_download(artifact, task.error_message)

                # POMæ–‡ä»¶ä¸‹è½½å¤±è´¥ç»Ÿè®¡
                if is_pom_file:
                    with self.pom_lock:
                        self.pom_stats['download_failed'] += 1
                    logger.error(f"[POM DEBUG] âŒ POM DOWNLOAD FAILED: {artifact.group_id}:{artifact.artifact_id}:{artifact.version} - {task.error_message}")

        except Exception as e:
            task.error_message = str(e)
            self.stats['download_failed'] += 1
            self.failed_tasks.append(task)

            # è®°å½•ä¸‹è½½å¤±è´¥åˆ°æ—¥å¿—æ–‡ä»¶
            self._log_failed_download(artifact, task.error_message)

            logger.error(f"Failed to download {artifact.file_path}: {e}")

        # ç¡®ä¿åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æ¸…ç†å†…å­˜
        if hasattr(task, 'file_data') and task.file_data is not None:
            with self.memory_lock:
                self.current_memory_usage -= len(task.file_data)
            logger.debug(f"Cleaned up memory: {len(task.file_data)} bytes, current usage: {self.current_memory_usage}")
            task.file_data = None

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({"down": self.stats['downloaded'],
                                   "up": self.stats['uploaded'],
                                   "skip": self.stats['skipped_existing']})
        progress_bar.update(1)

        return task.download_success

    def _emergency_memory_cleanup(self) -> None:
        """ç´§æ€¥å†…å­˜æ¸…ç†ï¼šæ¸…ç†å¤±è´¥çš„ä»»åŠ¡å†…å­˜"""
        try:
            with self.memory_lock:
                # æ¸…ç†å¤±è´¥çš„ä»»åŠ¡å†…å­˜
                cleared_memory = 0
                for task in self.failed_tasks[:]:  # å¤åˆ¶åˆ—è¡¨ï¼Œé¿å…ä¿®æ”¹æ—¶å‡ºé”™
                    if hasattr(task, 'file_data') and task.file_data is not None:
                        file_size = len(task.file_data)
                        self.current_memory_usage -= file_size
                        cleared_memory += file_size
                        task.file_data = None

                # æ¸…ç†ä¸Šä¼ é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡å†…å­˜
                temp_tasks = []
                while not self.upload_queue.empty():
                    try:
                        task = self.upload_queue.get_nowait()
                        if hasattr(task, 'file_data') and task.file_data is not None:
                            file_size = len(task.file_data)
                            self.current_memory_usage -= file_size
                            cleared_memory += file_size
                            task.file_data = None
                        temp_tasks.append(task)
                    except:
                        break

                # å°†ä»»åŠ¡æ”¾å›é˜Ÿåˆ—ï¼ˆæ²¡æœ‰å†…å­˜æ•°æ®çš„ï¼‰
                for task in temp_tasks:
                    try:
                        self.upload_queue.put_nowait(task)
                    except:
                        pass

                if cleared_memory > 0:
                    logger.warning(f"Emergency cleanup: released {cleared_memory} bytes, current usage: {self.current_memory_usage}")

        except Exception as e:
            logger.error(f"Error during emergency memory cleanup: {e}")

    def _check_and_cleanup_memory(self) -> None:
        """å®šæœŸæ£€æŸ¥å’Œæ¸…ç†å†…å­˜"""
        try:
            with self.memory_lock:
                memory_usage_mb = self.current_memory_usage / (1024 * 1024)
                limit_mb = self.max_memory_usage / (1024 * 1024)

                # å¦‚æœå†…å­˜ä½¿ç”¨è¶…è¿‡80%çš„é˜ˆå€¼ï¼Œè¿›è¡Œé¢„é˜²æ€§æ¸…ç†
                if memory_usage_mb > limit_mb * 0.8:
                    logger.info(f"Memory usage high ({memory_usage_mb:.1f}MB/{limit_mb:.1f}MB), performing preventive cleanup")
                    self._preventive_memory_cleanup()

                # æ¯5åˆ†é’Ÿè®°å½•ä¸€æ¬¡å†…å­˜ç»Ÿè®¡
                if hasattr(self, '_last_stats_log'):
                    if time.time() - self._last_stats_log > 300:
                        logger.info(f"Memory statistics: {memory_usage_mb:.1f}MB/{limit_mb:.1f}MB, "
                                   f"Queue size: {self.upload_queue.qsize()}, "
                                   f"Failed tasks: {len(self.failed_tasks)}")
                        self._last_stats_log = time.time()
                else:
                    self._last_stats_log = time.time()

        except Exception as e:
            logger.error(f"Error during memory check: {e}")

    def _preventive_memory_cleanup(self) -> None:
        """é¢„é˜²æ€§å†…å­˜æ¸…ç†ï¼šæ¸…ç†æ—§çš„ä»»åŠ¡å†…å­˜"""
        try:
            cleared_memory = 0

            # æ¸…ç†ä¸Šä¼ é˜Ÿåˆ—ä¸­ç­‰å¾…æ—¶é—´è¿‡é•¿çš„ä»»åŠ¡ï¼ˆè¶…è¿‡5åˆ†é’Ÿï¼‰
            current_time = time.time()
            temp_tasks = []
            while not self.upload_queue.empty():
                try:
                    task = self.upload_queue.get_nowait()
                    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦ç­‰å¾…æ—¶é—´è¿‡é•¿
                    if hasattr(task, 'file_data') and task.file_data is not None:
                        # å¦‚æœä»»åŠ¡åˆ›å»ºæ—¶é—´è¶…è¿‡5åˆ†é’Ÿï¼Œæ¸…ç†å†…å­˜ä½†ä¿ç•™ä»»åŠ¡
                        if hasattr(task, 'created_time'):
                            if current_time - task.created_time > 300:  # 5åˆ†é’Ÿ
                                file_size = len(task.file_data)
                                self.current_memory_usage -= file_size
                                cleared_memory += file_size
                                task.file_data = None
                                logger.debug(f"Preventive cleanup: released {file_size} bytes from old task")
                        else:
                            # æ²¡æœ‰åˆ›å»ºæ—¶é—´ä¿¡æ¯ï¼Œä¹Ÿæ¸…ç†ä»¥é˜²ä¸‡ä¸€
                            file_size = len(task.file_data)
                            self.current_memory_usage -= file_size
                            cleared_memory += file_size
                            task.file_data = None
                            logger.debug(f"Preventive cleanup: released {file_size} bytes from unknown task")
                    temp_tasks.append(task)
                except:
                    break

            # å°†ä»»åŠ¡æ”¾å›é˜Ÿåˆ—
            for task in temp_tasks:
                try:
                    self.upload_queue.put_nowait(task)
                except:
                    pass

            if cleared_memory > 0:
                logger.info(f"Preventive cleanup: released {cleared_memory} bytes")

        except Exception as e:
            logger.error(f"Error during preventive memory cleanup: {e}")

    def _download_artifact_simple(self, artifact: MavenArtifact) -> bool:
        """
        ç®€åŒ–çš„åˆ¶å“ä¸‹è½½æ–¹æ³•ï¼ˆç”¨äºç»„ä»¶è¿ç§»ï¼‰

        Args:
            artifact: åˆ¶å“ä¿¡æ¯

        Returns:
            ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        if self.stop_event.is_set():
            return False

        try:
            # ä¸‹è½½åˆ°å†…å­˜
            file_data = self._download_to_memory(artifact)
            if not file_data:
                logger.error(f"Failed to download {artifact.file_path}")
                self.stats['download_failed'] += 1
                return False

            # åˆ›å»ºå†…å­˜ä»»åŠ¡å¯¹è±¡
            task = MemoryMigrationTask(
                artifact=artifact,
                file_data=file_data,
                download_success=True
            )

            # æ·»åŠ åˆ°ä¸Šä¼ é˜Ÿåˆ—
            self.upload_queue.put(task)
            self.stats['downloaded'] += 1
            logger.debug(f"Successfully downloaded and queued: {artifact.file_path}")
            return True

        except Exception as e:
            logger.error(f"Download failed for {artifact.file_path}: {e}")
            self.stats['download_failed'] += 1
            self._log_failed_download(artifact, str(e))
            return False

    def _download_to_memory(self, artifact: MavenArtifact) -> Optional[bytes]:
        """ä¸‹è½½æ–‡ä»¶åˆ°å†…å­˜"""
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•é¿å…æƒé™é—®é¢˜
            temp_dir = tempfile.mkdtemp()
            temp_file_path = os.path.join(temp_dir, f"temp_{hash(artifact.file_path)}.tmp")

            try:
                success = self.coding_client.download_artifact(
                    artifact.file_path.split('/')[0],  # project_name from file_path
                    "releases" if "releases" in artifact.file_path else "snapshots",
                    artifact.file_path,
                    temp_file_path,
                    getattr(artifact, 'download_url', None)
                )

                if success and os.path.exists(temp_file_path):
                    # è¯»å–æ–‡ä»¶åˆ°å†…å­˜
                    with open(temp_file_path, 'rb') as f:
                        file_data = f.read()
                    return file_data
                else:
                    return None

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
                try:
                    if os.path.exists(temp_file_path):
                        os.unlink(temp_file_path)
                    os.rmdir(temp_dir)
                except Exception:
                    pass  # å¿½ç•¥æ¸…ç†é”™è¯¯

        except Exception as e:
            logger.error(f"Failed to download {artifact.file_path} to memory: {e}")
            return None

    def _upload_worker(self) -> None:
        """ä¸Šä¼ å·¥ä½œçº¿ç¨‹"""
        logger.info("Memory upload worker started")

        # è®°å½•ä¿å­˜è®¡æ•°å™¨å’ŒçŠ¶æ€æŠ¥å‘Š
        save_counter = 0
        status_counter = 0

        while not self.stop_event.is_set():
            try:
                # å®šæœŸæŠ¥å‘ŠçŠ¶æ€ï¼ˆæ¯30ç§’ï¼‰
                status_counter += 1
                if status_counter % 300 == 0:  # 300 * 0.1s = 30s
                    logger.info(f"Upload worker status - Memory usage: {self.current_memory_usage}/{self.max_memory_usage} bytes, "
                               f"Queue size: {self.upload_queue.qsize()}, Uploaded: {self.stats['uploaded']}, "
                               f"Failed: {self.stats['upload_failed']}")

                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                task = self.upload_queue.get(timeout=1.0)

                # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸæ ‡è®°
                if task is None:
                    logger.debug("Upload worker received shutdown signal")
                    break

                try:
                    if task.download_success and task.file_data:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºPOMæ–‡ä»¶
                        is_pom_file = task.artifact.file_path.endswith('.pom') or task.artifact.packaging == 'pom'
                        if is_pom_file:
                            with self.pom_lock:
                                self.pom_stats['upload_attempted'] += 1
                            logger.info(f"[POM DEBUG] ğŸ”„ STARTING POM UPLOAD: {task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version}")

                        # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œæ–‡ä»¶ç”¨äºä¸Šä¼ 
                        temp_dir = tempfile.mkdtemp()
                        temp_file_path = os.path.join(temp_dir, f"upload_{hash(task.artifact.file_path)}.tmp")

                        try:
                            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                            with open(temp_file_path, 'wb') as temp_file:
                                temp_file.write(task.file_data)
                                temp_file.flush()

                            # ä¸Šä¼ æ–‡ä»¶
                            maven_path = self._convert_to_maven_path(task.artifact)
                            logger.debug(f"[POM DEBUG] Uploading to Nexus path: {maven_path}")
                            result = self.nexus_uploader.upload_file(
                                Path(temp_file_path), maven_path
                            )

                            if result.get('success'):
                                task.upload_success = True
                                self.stats['uploaded'] += 1

                                # POMæ–‡ä»¶ä¸Šä¼ æˆåŠŸç»Ÿè®¡
                                if is_pom_file:
                                    with self.pom_lock:
                                        self.pom_stats['upload_success'] += 1
                                    logger.info(f"[POM DEBUG] âœ… POM UPLOAD SUCCESS: {task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version}")
                                    logger.info(f"[POM DEBUG]   Maven path: {maven_path}")
                                    logger.info(f"[POM DEBUG]   File size: {len(task.file_data)} bytes")
                                elif task.artifact.file_path.endswith('.pom'):
                                    with self.pom_lock:
                                        self.pom_stats['upload_success'] += 1
                                    logger.info(f"[POM DEBUG] âœ… POM UPLOAD SUCCESS: {task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version}")

                                # è®°å½•å·²ä¸Šä¼ çš„ä¾èµ–ä¿¡æ¯
                                repository_name = task.artifact.repository or "Unknown"
                                file_name = task.artifact.file_path.split('/')[-1] if task.artifact.file_path else "Unknown"
                                dependency_info = {
                                    'group_id': task.artifact.group_id,
                                    'artifact_id': task.artifact.artifact_id,
                                    'version': task.artifact.version,
                                    'packaging': task.artifact.packaging,
                                    'repository': repository_name,
                                    'filename': file_name,
                                    'upload_time': time.time()
                                }
                                self.uploaded_dependencies.append(dependency_info)

                                # è®°å½•å·²ä¸Šä¼ çš„æ–‡ä»¶ - ä½¿ç”¨Mavenåæ ‡å“ˆå¸Œè€Œä¸æ˜¯æ–‡ä»¶å†…å®¹å“ˆå¸Œ
                                identifier = f"{task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version}:{task.artifact.packaging}"
                                maven_hash = hashlib.md5(identifier.encode()).hexdigest()
                                self.uploaded_hashes.add(maven_hash)

                                # æ¸…æ™°æ˜¾ç¤ºä¸Šä¼ æˆåŠŸçš„ä¾èµ–
                                logger.info(f"[OK] UPLOADED DEPENDENCY: {task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version} ({task.artifact.packaging})")
                                logger.info(f"   Repository: {repository_name}")
                                logger.info(f"   Filename: {file_name}")

                                # å®šæœŸä¿å­˜è®°å½•ï¼ˆæ¯10ä¸ªä¸Šä¼ ä¿å­˜ä¸€æ¬¡ï¼‰
                                save_counter += 1
                                if save_counter % 10 == 0:
                                    self._save_migration_records()
                            else:
                                task.error_message = result.get('error', 'Upload failed')
                                self.stats['upload_failed'] += 1
                                self.failed_tasks.append(task)

                                # è®°å½•ä¸Šä¼ å¤±è´¥åˆ°æ—¥å¿—æ–‡ä»¶
                                self._log_failed_upload(task.artifact, maven_path, task.error_message)

                                # POMæ–‡ä»¶ä¸Šä¼ å¤±è´¥ç»Ÿè®¡
                                if is_pom_file:
                                    with self.pom_lock:
                                        self.pom_stats['upload_failed'] += 1
                                    logger.error(f"[POM DEBUG] âŒ POM UPLOAD FAILED: {task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version}")
                                    logger.error(f"[POM DEBUG]   Error: {task.error_message}")
                                    logger.error(f"[POM DEBUG]   Maven path: {maven_path}")

                        finally:
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
                            try:
                                if os.path.exists(temp_file_path):
                                    os.unlink(temp_file_path)
                                os.rmdir(temp_dir)
                            except Exception:
                                pass  # å¿½ç•¥æ¸…ç†é”™è¯¯

                except Exception as e:
                    task.error_message = str(e)
                    self.stats['upload_failed'] += 1
                    self.failed_tasks.append(task)

                    # è®°å½•ä¸Šä¼ å¤±è´¥åˆ°æ—¥å¿—æ–‡ä»¶
                    maven_path = self._convert_to_maven_path(task.artifact)
                    self._log_failed_upload(task.artifact, maven_path, task.error_message)

                    # POMæ–‡ä»¶å¼‚å¸¸å¤±è´¥ç»Ÿè®¡
                    is_pom_file = task.artifact.file_path.endswith('.pom') or task.artifact.packaging == 'pom'
                    if is_pom_file:
                        with self.pom_lock:
                            self.pom_stats['upload_failed'] += 1
                        logger.error(f"[POM DEBUG] âŒ POM UPLOAD EXCEPTION: {task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version} - {e}")
                    logger.error(f"Failed to upload {task.artifact.file_path}: {e}")

                finally:
                    # é‡Šæ”¾å†…å­˜ï¼ˆé‡è¦ï¼šç¡®ä¿æ€»æ˜¯é‡Šæ”¾å†…å­˜ï¼‰
                    if task.file_data:
                        file_size = len(task.file_data)
                        with self.memory_lock:
                            self.current_memory_usage -= file_size
                        logger.debug(f"Released {file_size} bytes from memory, current usage: {self.current_memory_usage}")
                        task.file_data = None

                    # æ ‡è®°ä»»åŠ¡å®Œæˆ
                    try:
                        self.upload_queue.task_done()
                    except Exception as task_done_error:
                        logger.error(f"Failed to mark task as done: {task_done_error}")

            except Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
                continue
            except Exception as e:
                logger.error(f"Upload worker error: {e}")
                # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.1)

        logger.info("Memory upload worker stopped - Final memory usage: {self.current_memory_usage} bytes")

    def _convert_to_maven_path(self, artifact: MavenArtifact) -> str:
        """è½¬æ¢ä¸º Maven è·¯å¾„æ ¼å¼"""
        parts = artifact.file_path.split('/')
        if len(parts) >= 4:
            group_id = '.'.join(parts[:-3])
            artifact_id = parts[-3]
            version = parts[-2]
            filename = parts[-1]

            return f"{group_id.replace('.', '/')}/{artifact_id}/{version}/{filename}"

        return artifact.file_path

    def _generate_final_stats(self) -> None:
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        logger.info("=" * 60)
        logger.info("MEMORY PIPELINE MIGRATION SUMMARY")
        logger.info("=" * 60)
        logger.info(f"[OK] Total artifacts processed: {self.stats['total_artifacts']}")
        logger.info(f"â¬‡ï¸  Downloaded: {self.stats['downloaded']}")
        logger.info(f"â¬†ï¸  Uploaded: {self.stats['uploaded']}")
        logger.info(f"â­ï¸  Skipped (already uploaded): {self.stats['skipped_existing']}")
        logger.info(f"[ERROR] Download failed: {self.stats['download_failed']}")
        logger.info(f"[ERROR] Upload failed: {self.stats['upload_failed']}")
        logger.info("=" * 60)

        # POM æ–‡ä»¶ç»Ÿè®¡
        pom_uploaded = len([dep for dep in self.uploaded_dependencies if dep['packaging'] == 'pom'])
        logger.info(f"POM files uploaded: {pom_uploaded}")

        if self.failed_tasks:
            # ä¸“é—¨ç»Ÿè®¡ POM æ–‡ä»¶å¤±è´¥
            pom_failed = [task for task in self.failed_tasks if task.artifact.packaging == 'pom']
            if pom_failed:
                logger.warning(f"POM files failed: {len(pom_failed)}")
                for task in pom_failed[:3]:  # æ˜¾ç¤ºå‰3ä¸ªPOMé”™è¯¯
                    logger.warning(f"  POM - {task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version} - {task.error_message}")

        processed = self.stats['uploaded'] + self.stats['upload_failed']
        success_rate = (self.stats['uploaded'] / processed * 100) if processed > 0 else 0
        logger.info(f"Upload success rate: {success_rate:.1f}%")

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„ POM æ–‡ä»¶
        jar_uploaded = len([dep for dep in self.uploaded_dependencies if dep['packaging'] == 'jar'])
        missing_poms = []
        for dep in self.uploaded_dependencies:
            if dep['packaging'] == 'jar':
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„ POM æ–‡ä»¶
                matching_pom = any(p['group_id'] == dep['group_id'] and
                                  p['artifact_id'] == dep['artifact_id'] and
                                  p['version'] == dep['version'] and
                                  p['packaging'] == 'pom'
                                  for p in self.uploaded_dependencies)
                if not matching_pom:
                    missing_poms.append(f"{dep['group_id']}:{dep['artifact_id']}:{dep['version']}")

        if missing_poms:
            logger.warning(f"âš ï¸  Found {len(missing_poms)} JAR files without corresponding POM files:")
            for missing in missing_poms[:5]:  # æ˜¾ç¤ºå‰5ä¸ªç¼ºå¤±çš„POM
                logger.warning(f"  - {missing}")
            if len(missing_poms) > 5:
                logger.warning(f"  ... and {len(missing_poms) - 5} more")

            # æ›´æ–°POMç»Ÿè®¡ä¸­çš„ç¼ºå¤±åˆ—è¡¨
            with self.pom_lock:
                self.pom_stats['missing_in_nexus'] = missing_poms

    def _display_pom_detailed_report(self) -> None:
        """æ˜¾ç¤ºPOMæ–‡ä»¶è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š"""
        logger.info("")
        logger.info("ğŸ“‹ POM FILE DETAILED REPORT")
        logger.info("=" * 80)

        with self.pom_lock:
            stats = self.pom_stats.copy()

        logger.info(f"[DISCOVERY] POM files found in CODING: {stats['discovered_in_coding']}")
        logger.info(f"[SKIPPED] POM files already uploaded: {stats['skipped_already_uploaded']}")
        logger.info(f"[DOWNLOAD] POM files download attempted: {stats['download_attempted']}")
        logger.info(f"[DOWNLOAD] POM files download success: {stats['download_success']}")
        logger.info(f"[DOWNLOAD] POM files download failed: {stats['download_failed']}")
        logger.info(f"[UPLOAD] POM files upload attempted: {stats['upload_attempted']}")
        logger.info(f"[UPLOAD] POM files upload success: {stats['upload_success']}")
        logger.info(f"[UPLOAD] POM files upload failed: {stats['upload_failed']}")

        # è®¡ç®—æˆåŠŸç‡
        if stats['download_attempted'] > 0:
            download_success_rate = (stats['download_success'] / stats['download_attempted']) * 100
            logger.info(f"[RATE] POM download success rate: {download_success_rate:.1f}%")

        if stats['upload_attempted'] > 0:
            upload_success_rate = (stats['upload_success'] / stats['upload_attempted']) * 100
            logger.info(f"[RATE] POM upload success rate: {upload_success_rate:.1f}%")

        # åˆ†æé—®é¢˜
        logger.info("")
        logger.info("ğŸ” POM FILE ANALYSIS:")

        if stats['discovered_in_coding'] == 0:
            logger.warning("âš ï¸  No POM files discovered in CODING repositories")
            logger.warning("   This could indicate:")
            logger.warning("   - No Maven projects with POM files in the repositories")
            logger.warning("   - POM files are filtered out by maven_filter configuration")
            logger.warning("   - API access issues preventing POM file discovery")
        elif stats['download_attempted'] < stats['discovered_in_coding']:
            skipped_count = stats['discovered_in_coding'] - stats['download_attempted'] - stats['skipped_already_uploaded']
            if skipped_count > 0:
                logger.warning(f"âš ï¸  {skipped_count} POM files discovered but not attempted for download")
                logger.warning("   This could indicate:")
                logger.warning("   - Files were incorrectly marked as already uploaded")
                logger.warning("   - Memory or processing issues prevented download attempts")

        if stats['download_failed'] > 0:
            failure_rate = (stats['download_failed'] / stats['download_attempted']) * 100 if stats['download_attempted'] > 0 else 0
            logger.warning(f"âš ï¸  {stats['download_failed']} POM files failed to download ({failure_rate:.1f}% failure rate)")
            logger.warning("   Possible causes:")
            logger.warning("   - Authentication issues with CODING Maven repositories")
            logger.warning("   - Network connectivity problems")
            logger.warning("   - POM files were deleted or moved in CODING")
            logger.warning("   - Temporary API errors or rate limiting")

        if stats['upload_failed'] > 0:
            failure_rate = (stats['upload_failed'] / stats['upload_attempted']) * 100 if stats['upload_attempted'] > 0 else 0
            logger.warning(f"âš ï¸  {stats['upload_failed']} POM files failed to upload ({failure_rate:.1f}% failure rate)")
            logger.warning("   Possible causes:")
            logger.warning("   - Nexus repository connection issues")
            logger.warning("   - Insufficient permissions in Nexus")
            logger.warning("   - Nexus repository configuration problems")
            logger.warning("   - Network issues between migration tool and Nexus")

        # ç¼ºå¤±çš„POMæ–‡ä»¶åˆ†æ
        if stats['missing_in_nexus']:
            logger.warning(f"âš ï¸  {len(stats['missing_in_nexus'])} JAR files in Nexus are missing corresponding POM files:")
            for missing in stats['missing_in_nexus'][:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                logger.warning(f"  - {missing}")
            if len(stats['missing_in_nexus']) > 10:
                logger.warning(f"  ... and {len(stats['missing_in_nexus']) - 10} more")

            logger.warning("")
            logger.warning("   This indicates that POM files:")
            logger.warning("   - May not exist in CODING (check discovery count above)")
            logger.warning("   - Failed to download from CODING")
            logger.warning("   - Failed to upload to Nexus")
            logger.warning("   - Were processed but had errors during migration")

        logger.info("=" * 80)
        logger.info("")

        # å»ºè®®çš„è°ƒè¯•æ­¥éª¤
        if stats['discovered_in_coding'] > 0 and (stats['download_failed'] > 0 or stats['upload_failed'] > 0 or len(stats['missing_in_nexus']) > 0):
            logger.info("ğŸ”§ SUGGESTED DEBUGGING STEPS:")
            logger.info("1. Check the detailed [POM DEBUG] logs above for specific error messages")
            logger.info("2. Review failed dependency logs:")
            logger.info(f"   - CODING download failures: {self.failed_download_log}")
            logger.info(f"   - Nexus upload failures: {self.failed_upload_log}")
            logger.info("3. Verify CODING Maven repository authentication in config.yaml")
            logger.info("4. Test manual access to CODING Maven repository URLs")
            logger.info("5. Check Nexus repository permissions and configuration")
            logger.info("6. Verify network connectivity to both CODING and Nexus")
            logger.info("7. Consider running with logging level set to DEBUG for more details")
            logger.info("")

    def migrate_components(self, components: List[dict]) -> Dict[str, Any]:
        """
        è¿ç§»æŒ‡å®šçš„ç»„ä»¶

        Args:
            components: ç»„ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªç»„ä»¶åŒ…å« group_id, artifact_id, version

        Returns:
            è¿ç§»ç»“æœç»Ÿè®¡
        """
        logger.info(f"Starting memory pipeline migration for {len(components)} components")

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_artifacts': 0,
            'downloaded': 0,
            'uploaded': 0,
            'download_failed': 0,
            'upload_failed': 0,
            'skipped_existing': 0
        }

        try:
            # ä¸ºç»„ä»¶è¿ç§»åˆ›å»ºä¸“é—¨çš„è®°å½•æ–‡ä»¶
            import time
            timestamp = int(time.time())
            self.record_file = self.records_dir / f"components_migration_{timestamp}.json"
            self._load_migration_records()

            # è·å–æ‰€æœ‰é¡¹ç›®ä»¥ç”¨äºæŸ¥æ‰¾ç»„ä»¶
            all_projects = self.coding_client.get_all_projects()
            if not all_projects:
                logger.error("[ERROR] æ— æ³•è·å–ä»»ä½•é¡¹ç›®ä¿¡æ¯")
                return stats

            # è·å–æ¯ä¸ªé¡¹ç›®çš„ä»“åº“ä¿¡æ¯
            all_artifacts = []

            for component in components:
                group_id = component['group_id']
                artifact_id = component['artifact_id']
                version = component['version']
                package_name = f"{group_id}:{artifact_id}"

                logger.info(f"[SEARCH] æŸ¥æ‰¾ç»„ä»¶: {package_name}:{version}")

                component_found = False

                # åœ¨æ‰€æœ‰é¡¹ç›®ä¸­æŸ¥æ‰¾è¿™ä¸ªç»„ä»¶
                for project in all_projects:
                    logger.debug(f"[SEARCH] åœ¨é¡¹ç›® {project.name} (ID: {project.id}) ä¸­æŸ¥æ‰¾...")

                    # è·å–é¡¹ç›®çš„ä»“åº“ä¿¡æ¯
                    repositories = []
                    try:
                        repos = self.coding_client.get_artifact_repositories(project.id)
                        for repo in repos:
                            if repo.get('Type') == 3:  # Maven ç±»å‹
                                repositories.append(repo.get('Name'))
                        logger.debug(f"[SEARCH] é¡¹ç›® {project.name} æ‰¾åˆ° Maven ä»“åº“: {repositories}")
                    except Exception as e:
                        logger.debug(f"[SEARCH] è·å–é¡¹ç›® {project.name} ä»“åº“ä¿¡æ¯å¤±è´¥: {e}")
                        continue

                    # åœ¨æ¯ä¸ªä»“åº“ä¸­æŸ¥æ‰¾ç»„ä»¶
                    for repository_name in repositories:
                        try:
                            logger.debug(f"[SEARCH] åœ¨ä»“åº“ {repository_name} ä¸­æŸ¥æ‰¾ {package_name}:{version}")

                            # è°ƒç”¨ get_maven_version_files è·å–ç»„ä»¶çš„æ–‡ä»¶åˆ—è¡¨
                            artifacts = self.coding_client.get_maven_version_files(
                                project.id, project.name, repository_name, package_name, version
                            )
                            logger.info(f"[SEARCH] ä»“åº“ {repository_name} è¿”å›äº† {len(artifacts) if artifacts else 0} ä¸ªåˆ¶å“")

                            if artifacts:
                                logger.info(f"[SUCCESS] åœ¨é¡¹ç›® {project.name} çš„ä»“åº“ {repository_name} ä¸­æ‰¾åˆ° {len(artifacts)} ä¸ªæ–‡ä»¶")
                                logger.info(f"[INFO] æ–‡ä»¶åˆ—è¡¨: {[a.download_url.split('/')[-1] for a in artifacts[:5]]}")

                                # æ·»åŠ é¡¹ç›®åç§°åˆ°åˆ¶å“ä¿¡æ¯ä¸­
                                for artifact in artifacts:
                                    # ä¸ºåˆ¶å“å¯¹è±¡æ·»åŠ é¡¹ç›®ä¿¡æ¯
                                    artifact.project_name = project.name
                                    artifact.project_id = project.id
                                    # å¦‚æœä»“åº“ä¿¡æ¯ä¸ºç©ºï¼Œåˆ™è®¾ç½®
                                    if not artifact.repository:
                                        artifact.repository = repository_name

                                all_artifacts.extend(artifacts)
                                component_found = True
                                logger.info(f"[SUCCESS] ç»„ä»¶ {package_name}:{version} æŸ¥æ‰¾æˆåŠŸï¼Œå…±æ‰¾åˆ° {len(all_artifacts)} ä¸ªæ–‡ä»¶")
                                logger.info(f"[DEBUG] è®¾ç½® component_found = Trueï¼Œå‡†å¤‡è·³å‡ºä»“åº“å¾ªç¯")

                                # ç»„ä»¶åœ¨ä¸€ä¸ªé¡¹ç›®ä¸­æ‰¾åˆ°åï¼Œå°±ä¸åœ¨å…¶ä»–é¡¹ç›®ä¸­ç»§ç»­æŸ¥æ‰¾
                                break

                        except Exception as e:
                            logger.debug(f"[SEARCH] åœ¨ä»“åº“ {repository_name} ä¸­æŸ¥æ‰¾å¤±è´¥: {e}")
                            continue

                    logger.info(f"[DEBUG] å®Œæˆé¡¹ç›® {project.name} çš„æ‰€æœ‰ä»“åº“æŸ¥æ‰¾ï¼Œcomponent_found = {component_found}")
                    if component_found:
                        logger.info(f"[DEBUG] component_found = Trueï¼Œå‡†å¤‡è·³å‡ºé¡¹ç›®å¾ªç¯")
                        break

                if not component_found:
                    logger.warning(f"[NOT FOUND] ç»„ä»¶ {package_name}:{version} åœ¨æ‰€æœ‰é¡¹ç›®ä¸­éƒ½æœªæ‰¾åˆ°")

            stats['total_artifacts'] = len(all_artifacts)

            if not all_artifacts:
                logger.warning("[WARNING] æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»„ä»¶æ–‡ä»¶")
                logger.info("[INFO] å¯èƒ½çš„åŸå› :")
                logger.info("  1. ç»„ä»¶åæ ‡ä¸æ­£ç¡®")
                logger.info("  2. ç‰ˆæœ¬å·ä¸åŒ¹é…")
                logger.info("  3. ç»„ä»¶ä¸åœ¨ä»»ä½• CODING Maven ä»“åº“ä¸­")
                logger.info("  4. ä»“åº“è®¿é—®æƒé™é—®é¢˜")
                return stats

            logger.info(f"[INFO] æ€»å…±æ‰¾åˆ° {len(all_artifacts)} ä¸ªæ–‡ä»¶å¾…è¿ç§»")
            logger.info(f"[INFO] æ–‡ä»¶é¢„è§ˆ: {[a.file_path for a in all_artifacts[:5]]}")  # æ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶è·¯å¾„

            # è¿‡æ»¤å·²ä¸Šä¼ çš„åˆ¶å“
            filtered_artifacts = []
            for artifact in all_artifacts:
                existing_hash = self._check_if_already_uploaded(artifact)
                if existing_hash:
                    stats['skipped_existing'] += 1
                    logger.debug(f"Skipping already uploaded: {artifact.file_path}")
                else:
                    filtered_artifacts.append(artifact)

            logger.info(f"Found {len(all_artifacts)} total artifacts, {len(filtered_artifacts)} to migrate "
                       f"({stats['skipped_existing']} already uploaded)")

            if not filtered_artifacts:
                logger.info("All artifacts have already been migrated")
                return stats

            # æ‰§è¡Œè¿ç§»é€»è¾‘ï¼ˆå¤ç”¨ç°æœ‰çš„æµæ°´çº¿é€»è¾‘ï¼‰
            # é‡ç½®é˜Ÿåˆ—å’Œç»Ÿè®¡
            self.upload_queue = Queue(maxsize=50)
            self.completed_tasks = []
            self.failed_tasks = []
            self.stats = stats.copy()
            self.stop_event.clear()

            # å¯åŠ¨ä¸‹è½½å’Œä¸Šä¼ çº¿ç¨‹æ± 
            with ThreadPoolExecutor(max_workers=self.download_workers + self.upload_workers) as executor:
                # å¯åŠ¨ä¸Šä¼ å·¥ä½œçº¿ç¨‹
                upload_futures = []
                for i in range(self.upload_workers):
                    future = executor.submit(self._upload_worker)
                    upload_futures.append(future)

                # å¯åŠ¨ä¸‹è½½å·¥ä½œçº¿ç¨‹
                download_futures = []
                for artifact in filtered_artifacts:
                    if self.stop_event.is_set():
                        break
                    future = executor.submit(self._download_artifact_simple, artifact)
                    download_futures.append(future)

                # ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ
                for future in as_completed(download_futures):
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"Download task failed: {e}")

                # æ·»åŠ ç»“æŸæ ‡è®°åˆ°ä¸Šä¼ é˜Ÿåˆ—
                for _ in range(self.upload_workers):
                    self.upload_queue.put(None)

                # ç­‰å¾…æ‰€æœ‰ä¸Šä¼ ä»»åŠ¡å®Œæˆ
                for future in as_completed(upload_futures):
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"Upload worker failed: {e}")

            # ä¿å­˜è¿ç§»è®°å½•
            self._save_migration_records()

            # è®°å½•æœ€ç»ˆç»Ÿè®¡
            final_stats = self.stats.copy()
            # ç¡®ä¿ total_artifacts åæ˜ å®é™…æ‰¾åˆ°çš„åˆ¶å“æ•°
            final_stats['total_artifacts'] = len(all_artifacts)
            logger.info(f"Component migration completed:")
            logger.info(f"  Total artifacts: {final_stats['total_artifacts']}")
            logger.info(f"  Downloaded: {final_stats['downloaded']}")
            logger.info(f"  Uploaded: {final_stats['uploaded']}")
            logger.info(f"  Skipped existing: {final_stats['skipped_existing']}")
            logger.info(f"  Download failed: {final_stats['download_failed']}")
            logger.info(f"  Upload failed: {final_stats['upload_failed']}")

            return final_stats

        except Exception as e:
            logger.error(f"Component migration failed: {e}")
            stats['error'] = str(e)
            return stats