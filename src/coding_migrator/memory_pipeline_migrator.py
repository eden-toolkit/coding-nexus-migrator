"""
å†…å­˜æµæµæ°´çº¿è¿ç§»å™¨ - é›¶ç£ç›˜å ç”¨çš„è¾¹ä¸‹è½½è¾¹ä¸Šä¼ è¿ç§»
"""

import os
import json
import hashlib
import logging
import threading
import tempfile
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue, Empty
from dataclasses import dataclass
from tqdm import tqdm
import time
import io

from .models import MavenArtifact, MigrationConfig
from .coding_client import CodingClient
from .nexus_uploader import NexusUploader


logger = logging.getLogger(__name__)


@dataclass
class MemoryMigrationTask:
    """å†…å­˜è¿ç§»ä»»åŠ¡"""
    artifact: MavenArtifact
    file_hash: Optional[str] = None
    download_success: bool = False
    upload_success: bool = False
    error_message: Optional[str] = None
    file_data: Optional[bytes] = None


class MemoryPipelineMigrator:
    """å†…å­˜æµæµæ°´çº¿è¿ç§»å™¨ - é›¶ç£ç›˜å ç”¨"""

    def __init__(self, config: MigrationConfig):
        """
        åˆå§‹åŒ–å†…å­˜æµæ°´çº¿è¿ç§»å™¨

        Args:
            config: è¿ç§»é…ç½®
        """
        self.config = config
        self.coding_client = CodingClient(
            config.coding_token,
            config.coding_team_id,
            config.maven_repositories,
            config.pagination,
            config.performance.max_workers,
            requests_per_second=config.rate_limit.requests_per_second
        )
        self.nexus_uploader = NexusUploader(config)

        # æ€§èƒ½é…ç½®
        self.download_workers = config.performance.max_workers
        self.upload_workers = min(config.performance.max_workers // 2, 5)

        # ä»»åŠ¡é˜Ÿåˆ—
        self.upload_queue = Queue(maxsize=50)  # å‡å°é˜Ÿåˆ—å¤§å°ï¼Œå‡å°‘å†…å­˜å ç”¨
        self.completed_tasks = []
        self.failed_tasks = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_artifacts': 0,
            'downloaded': 0,
            'uploaded': 0,
            'download_failed': 0,
            'upload_failed': 0,
            'skipped_existing': 0
        }

        # æ§åˆ¶æ ‡å¿—
        self.stop_event = threading.Event()

        # è¿ç§»è®°å½•ç›®å½•ï¼ˆå›ºå®šç›®å½•ï¼Œæ”¯æŒå¢é‡è¿ç§»ï¼‰
        self.records_dir = Path("target")
        self.records_dir.mkdir(exist_ok=True)

        # è¿ç§»è®°å½•æ–‡ä»¶å’Œä¾èµ–åˆ—è¡¨ï¼ˆå°†åœ¨migrate_projectä¸­åˆå§‹åŒ–ï¼‰
        self.record_file = None
        self.uploaded_hashes: Set[str] = set()
        self.uploaded_dependencies = []

        # å†…å­˜é™åˆ¶ï¼ˆå•ä½ï¼šå­—èŠ‚ï¼‰
        self.max_memory_usage = 100 * 1024 * 1024  # 100MB
        self.current_memory_usage = 0
        self.memory_lock = threading.Lock()

    def _load_migration_records(self) -> None:
        """åŠ è½½å·²è¿ç§»è®°å½•"""
        try:
            if self.record_file.exists():
                with open(self.record_file, 'r', encoding='utf-8') as f:
                    records = json.load(f)
                    self.uploaded_hashes = set(records.get('uploaded_hashes', []))
                    self.uploaded_dependencies = records.get('uploaded_dependencies', [])
                logger.info(f"Loaded {len(self.uploaded_hashes)} migration records")
                if self.uploaded_dependencies:
                    logger.info(f"Previously uploaded {len(self.uploaded_dependencies)} dependencies")
        except Exception as e:
            logger.warning(f"Failed to load migration records: {e}")
            self.uploaded_hashes = set()
            self.uploaded_dependencies = []

    def _save_migration_records(self) -> None:
        """ä¿å­˜è¿ç§»è®°å½•"""
        try:
            records = {
                'uploaded_hashes': list(self.uploaded_hashes),
                'uploaded_dependencies': self.uploaded_dependencies,
                'last_updated': time.time()
            }
            with open(self.record_file, 'w', encoding='utf-8') as f:
                json.dump(records, f, indent=2, ensure_ascii=False)
            logger.debug(f"Saved {len(self.uploaded_hashes)} migration records")
        except Exception as e:
            logger.error(f"Failed to save migration records: {e}")

    def _calculate_file_hash(self, file_data: bytes) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        return hashlib.sha256(file_data).hexdigest()

    def _check_if_already_uploaded(self, artifact: MavenArtifact) -> Optional[str]:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»ä¸Šä¼ è¿‡"""
        # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
        identifier = f"{artifact.group_id}:{artifact.artifact_id}:{artifact.version}:{artifact.packaging}"
        hash_identifier = hashlib.md5(identifier.encode()).hexdigest()

        if hash_identifier in self.uploaded_hashes:
            return hash_identifier
        return None

    def migrate_project(self, project_id: int, project_name: str) -> Dict[str, Any]:
        """
        è¿ç§»å•ä¸ªé¡¹ç›®ï¼ˆå†…å­˜æµæ¨¡å¼ï¼‰

        Args:
            project_id: é¡¹ç›® ID
            project_name: é¡¹ç›®åç§°

        Returns:
            è¿ç§»ç»“æœç»Ÿè®¡
        """
        logger.info(f"Starting memory pipeline migration for project: {project_name}")

        try:
            # åˆå§‹åŒ–è¿ç§»è®°å½•æ–‡ä»¶ï¼ˆæŒ‰é¡¹ç›®å’Œé¡¹ç›®IDåŒºåˆ†ï¼‰
            self.record_file = self.records_dir / f"migration_records_{project_name}_{project_id}.json"
            self._load_migration_records()

            # è·å–æ‰€æœ‰åˆ¶å“
            all_artifacts = self._get_all_artifacts(project_id)
            self.stats['total_artifacts'] = len(all_artifacts)

            if not all_artifacts:
                logger.warning(f"No artifacts found for project: {project_name}")
                return self.stats

            # è¿‡æ»¤å·²ä¸Šä¼ çš„åˆ¶å“
            filtered_artifacts = []
            for artifact in all_artifacts:
                existing_hash = self._check_if_already_uploaded(artifact)
                if existing_hash:
                    self.stats['skipped_existing'] += 1
                    logger.debug(f"Skipping already uploaded: {artifact.file_path}")
                else:
                    filtered_artifacts.append(artifact)

            logger.info(f"Found {len(all_artifacts)} total artifacts, {len(filtered_artifacts)} to migrate "
                       f"({self.stats['skipped_existing']} already uploaded)")

            if not filtered_artifacts:
                logger.info("All artifacts have already been migrated")
                return self.stats

            # å¯åŠ¨ä¸‹è½½å’Œä¸Šä¼ çº¿ç¨‹æ± 
            with ThreadPoolExecutor(max_workers=self.download_workers + self.upload_workers) as executor:
                # å¯åŠ¨ä¸Šä¼ å·¥ä½œçº¿ç¨‹
                upload_futures = []
                for i in range(self.upload_workers):
                    future = executor.submit(self._upload_worker)
                    upload_futures.append(future)

                # å¯åŠ¨ä¸‹è½½ä»»åŠ¡
                download_futures = []
                progress_bar = tqdm(total=len(filtered_artifacts), desc="Memory Pipeline",
                                  unit="files", postfix={"down": 0, "up": 0, "skip": self.stats['skipped_existing']})

                for artifact in filtered_artifacts:
                    future = executor.submit(self._download_and_queue, artifact, progress_bar)
                    download_futures.append(future)

                # ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ
                for future in as_completed(download_futures):
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"Download task failed: {e}")

                # ç­‰å¾…ä¸Šä¼ é˜Ÿåˆ—æ¸…ç©º
                logger.info("Waiting for upload queue to empty...")
                queue_wait_count = 0
                max_queue_wait = 600  # æœ€å¤§ç­‰å¾…60ç§’
                while not self.upload_queue.empty() and queue_wait_count < max_queue_wait:
                    queue_wait_count += 1
                    if queue_wait_count % 50 == 0:  # æ¯5ç§’æ‰“å°ä¸€æ¬¡
                        logger.info(f"Upload queue size: {self.upload_queue.qsize()}, waiting... ({queue_wait_count * 0.1:.1f}s)")
                    time.sleep(0.1)

                if not self.upload_queue.empty():
                    logger.warning(f"Upload queue not empty after timeout, {self.upload_queue.qsize()} items remaining")

                # åœæ­¢ä¸Šä¼ å·¥ä½œçº¿ç¨‹
                self.stop_event.set()

                # ç­‰å¾…ä¸Šä¼ çº¿ç¨‹å®Œæˆ
                for future in upload_futures:
                    future.result()

                progress_bar.close()

        except Exception as e:
            logger.error(f"Memory pipeline migration failed: {e}")
        finally:
            # ä¿å­˜è¿ç§»è®°å½•
            self._save_migration_records()

        # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡
        self._generate_final_stats()

        # æ˜¾ç¤ºå·²ä¸Šä¼ çš„ä¾èµ–æ±‡æ€»
        self._display_uploaded_dependencies_summary()

        return self.stats

    def _generate_final_stats(self) -> None:
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š MEMORY PIPELINE MIGRATION SUMMARY")
        logger.info("=" * 60)
        logger.info(f"[OK] Total artifacts processed: {self.stats['total_artifacts']}")
        logger.info(f"â¬‡ï¸  Downloaded: {self.stats['downloaded']}")
        logger.info(f"â¬†ï¸  Uploaded: {self.stats['uploaded']}")
        logger.info(f"â­ï¸  Skipped (already uploaded): {self.stats['skipped_existing']}")
        logger.info(f"[ERROR] Download failed: {self.stats['download_failed']}")
        logger.info(f"[ERROR] Upload failed: {self.stats['upload_failed']}")
        logger.info("=" * 60)

    def _display_uploaded_dependencies_summary(self) -> None:
        """æ˜¾ç¤ºå·²ä¸Šä¼ ä¾èµ–çš„æ±‡æ€»ä¿¡æ¯"""
        if not self.uploaded_dependencies:
            logger.info("[INFO] No new dependencies uploaded in this session")
            return

        logger.info("")
        logger.info("[INFO] UPLOADED DEPENDENCIES SUMMARY")
        logger.info("=" * 60)

        # æŒ‰group_idå’Œartifact_idåˆ†ç»„æ˜¾ç¤º
        grouped_deps = {}
        for dep in self.uploaded_dependencies:
            key = f"{dep['group_id']}:{dep['artifact_id']}"
            if key not in grouped_deps:
                grouped_deps[key] = []
            grouped_deps[key].append(dep)

        for group_key, deps in sorted(grouped_deps.items()):
            logger.info(f"ğŸ“‹ {group_key}")
            for dep in sorted(deps, key=lambda x: x['version']):
                logger.info(f"   ğŸ·ï¸  {dep['version']} ({dep['packaging']}) - {dep['repository']}")

        logger.info("=" * 60)
        logger.info(f"ğŸ“ˆ Total dependencies uploaded: {len(self.uploaded_dependencies)}")
        logger.info("")

    def _get_all_artifacts(self, project_id: int) -> List[MavenArtifact]:
        """è·å–æ‰€æœ‰åˆ¶å“"""
        all_artifacts = []

        # è·å–é¡¹ç›®åç§°
        project_name = self.coding_client.get_project_name_by_id(project_id)

        # è·å–åˆ¶å“ä»“åº“
        repositories = self.coding_client.get_artifact_repositories(project_id)
        maven_repos = [repo for repo in repositories if repo.get('Type') == 3]

        for repo in maven_repos:
            repo_name = repo.get('Name', '')
            logger.info(f"Processing repository: {repo_name}")

            try:
                artifacts = self.coding_client.get_maven_artifacts(
                    project_id, repo_name, self.config.maven_filter
                )
                all_artifacts.extend(artifacts)
                logger.info(f"Found {len(artifacts)} artifacts in repository: {repo_name}")

            except Exception as e:
                logger.error(f"Failed to get artifacts from repository {repo_name}: {e}")

        return all_artifacts

    def _download_and_queue(self, artifact: MavenArtifact, progress_bar: tqdm) -> bool:
        """
        ä¸‹è½½åˆ¶å“åˆ°å†…å­˜å¹¶åŠ å…¥ä¸Šä¼ é˜Ÿåˆ—

        Args:
            artifact: åˆ¶å“ä¿¡æ¯
            progress_bar: è¿›åº¦æ¡

        Returns:
            ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        if self.stop_event.is_set():
            return False

        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸Šä¼ è¿‡
        existing_hash = self._check_if_already_uploaded(artifact)
        if existing_hash:
            logger.info(f"â­ï¸  SKIP: {artifact.group_id}:{artifact.artifact_id}:{artifact.version} already uploaded")
            self.stats['skipped_existing'] += 1
            progress_bar.set_postfix({"down": self.stats['downloaded'],
                                       "up": self.stats['uploaded'],
                                       "skip": self.stats['skipped_existing']})
            return True

        task = MemoryMigrationTask(artifact=artifact)

        try:
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨é™åˆ¶
            with self.memory_lock:
                if self.current_memory_usage > self.max_memory_usage:
                    logger.warning(f"Memory usage limit reached: {self.current_memory_usage}/{self.max_memory_usage} bytes, waiting...")
                    wait_count = 0
                    max_wait_time = 300  # æœ€å¤§ç­‰å¾…30ç§’
                    while self.current_memory_usage > self.max_memory_usage * 0.5:
                        wait_count += 1
                        if wait_count % 50 == 0:  # æ¯5ç§’æ‰“å°ä¸€æ¬¡
                            logger.info(f"Waiting for memory release: {self.current_memory_usage} bytes (waited {wait_count * 0.1:.1f}s)")
                        if wait_count >= max_wait_time:  # è¶…æ—¶ä¿æŠ¤
                            logger.error(f"Memory wait timeout after {max_wait_time * 0.1:.1f}s, forcing continue")
                            break
                        time.sleep(0.1)

            # ä¸‹è½½æ–‡ä»¶åˆ°å†…å­˜
            file_data = self._download_to_memory(artifact)

            if file_data:
                task.file_data = file_data
                task.file_hash = self._calculate_file_hash(file_data)
                task.download_success = True
                self.stats['downloaded'] += 1

                # æ›´æ–°å†…å­˜ä½¿ç”¨é‡
                with self.memory_lock:
                    self.current_memory_usage += len(file_data)

                # åŠ å…¥ä¸Šä¼ é˜Ÿåˆ—
                try:
                    self.upload_queue.put(task, timeout=30)
                except Exception as queue_error:
                    logger.error(f"Failed to add task to upload queue: {queue_error}")
                    task.error_message = f"Queue error: {queue_error}"
                    self.stats['upload_failed'] += 1
                    self.failed_tasks.append(task)
                    return False

                logger.debug(f"Downloaded and queued: {artifact.file_path} ({len(file_data)} bytes)")

            else:
                task.error_message = "Download failed"
                self.stats['download_failed'] += 1
                self.failed_tasks.append(task)

        except Exception as e:
            task.error_message = str(e)
            self.stats['download_failed'] += 1
            self.failed_tasks.append(task)
            logger.error(f"Failed to download {artifact.file_path}: {e}")

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({"down": self.stats['downloaded'],
                                   "up": self.stats['uploaded'],
                                   "skip": self.stats['skipped_existing']})
        progress_bar.update(1)

        return task.download_success

    def _download_to_memory(self, artifact: MavenArtifact) -> Optional[bytes]:
        """ä¸‹è½½æ–‡ä»¶åˆ°å†…å­˜"""
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•é¿å…æƒé™é—®é¢˜
            temp_dir = tempfile.mkdtemp()
            temp_file_path = os.path.join(temp_dir, f"temp_{hash(artifact.file_path)}.tmp")

            try:
                success = self.coding_client.download_artifact(
                    artifact.file_path.split('/')[0],  # project_name from file_path
                    "releases" if "releases" in artifact.file_path else "snapshots",
                    artifact.file_path,
                    temp_file_path,
                    getattr(artifact, 'download_url', None)
                )

                if success and os.path.exists(temp_file_path):
                    # è¯»å–æ–‡ä»¶åˆ°å†…å­˜
                    with open(temp_file_path, 'rb') as f:
                        file_data = f.read()
                    return file_data
                else:
                    return None

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
                try:
                    if os.path.exists(temp_file_path):
                        os.unlink(temp_file_path)
                    os.rmdir(temp_dir)
                except Exception:
                    pass  # å¿½ç•¥æ¸…ç†é”™è¯¯

        except Exception as e:
            logger.error(f"Failed to download {artifact.file_path} to memory: {e}")
            return None

    def _upload_worker(self) -> None:
        """ä¸Šä¼ å·¥ä½œçº¿ç¨‹"""
        logger.info("Memory upload worker started")

        # è®°å½•ä¿å­˜è®¡æ•°å™¨å’ŒçŠ¶æ€æŠ¥å‘Š
        save_counter = 0
        status_counter = 0

        while not self.stop_event.is_set():
            try:
                # å®šæœŸæŠ¥å‘ŠçŠ¶æ€ï¼ˆæ¯30ç§’ï¼‰
                status_counter += 1
                if status_counter % 300 == 0:  # 300 * 0.1s = 30s
                    logger.info(f"Upload worker status - Memory usage: {self.current_memory_usage}/{self.max_memory_usage} bytes, "
                               f"Queue size: {self.upload_queue.qsize()}, Uploaded: {self.stats['uploaded']}, "
                               f"Failed: {self.stats['upload_failed']}")

                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                task = self.upload_queue.get(timeout=1.0)

                try:
                    if task.download_success and task.file_data:
                        # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œæ–‡ä»¶ç”¨äºä¸Šä¼ 
                        temp_dir = tempfile.mkdtemp()
                        temp_file_path = os.path.join(temp_dir, f"upload_{hash(task.artifact.file_path)}.tmp")

                        try:
                            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                            with open(temp_file_path, 'wb') as temp_file:
                                temp_file.write(task.file_data)
                                temp_file.flush()

                            # ä¸Šä¼ æ–‡ä»¶
                            maven_path = self._convert_to_maven_path(task.artifact)
                            result = self.nexus_uploader.upload_file(
                                Path(temp_file_path), maven_path
                            )

                            if result.get('success'):
                                task.upload_success = True
                                self.stats['uploaded'] += 1

                                # è®°å½•å·²ä¸Šä¼ çš„ä¾èµ–ä¿¡æ¯
                                repository_name = task.artifact.repository or "Unknown"
                                file_name = task.artifact.file_path.split('/')[-1] if task.artifact.file_path else "Unknown"
                                dependency_info = {
                                    'group_id': task.artifact.group_id,
                                    'artifact_id': task.artifact.artifact_id,
                                    'version': task.artifact.version,
                                    'packaging': task.artifact.packaging,
                                    'repository': repository_name,
                                    'filename': file_name,
                                    'upload_time': time.time()
                                }
                                self.uploaded_dependencies.append(dependency_info)

                                # è®°å½•å·²ä¸Šä¼ çš„æ–‡ä»¶ - ä½¿ç”¨Mavenåæ ‡å“ˆå¸Œè€Œä¸æ˜¯æ–‡ä»¶å†…å®¹å“ˆå¸Œ
                                identifier = f"{task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version}:{task.artifact.packaging}"
                                maven_hash = hashlib.md5(identifier.encode()).hexdigest()
                                self.uploaded_hashes.add(maven_hash)

                                # æ¸…æ™°æ˜¾ç¤ºä¸Šä¼ æˆåŠŸçš„ä¾èµ–
                                logger.info(f"[OK] UPLOADED DEPENDENCY: {task.artifact.group_id}:{task.artifact.artifact_id}:{task.artifact.version} ({task.artifact.packaging})")
                                logger.info(f"   Repository: {repository_name}")
                                logger.info(f"   Filename: {file_name}")

                                # å®šæœŸä¿å­˜è®°å½•ï¼ˆæ¯10ä¸ªä¸Šä¼ ä¿å­˜ä¸€æ¬¡ï¼‰
                                save_counter += 1
                                if save_counter % 10 == 0:
                                    self._save_migration_records()
                            else:
                                task.error_message = result.get('error', 'Upload failed')
                                self.stats['upload_failed'] += 1
                                self.failed_tasks.append(task)

                        finally:
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
                            try:
                                if os.path.exists(temp_file_path):
                                    os.unlink(temp_file_path)
                                os.rmdir(temp_dir)
                            except Exception:
                                pass  # å¿½ç•¥æ¸…ç†é”™è¯¯

                except Exception as e:
                    task.error_message = str(e)
                    self.stats['upload_failed'] += 1
                    self.failed_tasks.append(task)
                    logger.error(f"Failed to upload {task.artifact.file_path}: {e}")

                finally:
                    # é‡Šæ”¾å†…å­˜ï¼ˆé‡è¦ï¼šç¡®ä¿æ€»æ˜¯é‡Šæ”¾å†…å­˜ï¼‰
                    if task.file_data:
                        file_size = len(task.file_data)
                        with self.memory_lock:
                            self.current_memory_usage -= file_size
                        logger.debug(f"Released {file_size} bytes from memory, current usage: {self.current_memory_usage}")
                        task.file_data = None

                    # æ ‡è®°ä»»åŠ¡å®Œæˆ
                    try:
                        self.upload_queue.task_done()
                    except Exception as task_done_error:
                        logger.error(f"Failed to mark task as done: {task_done_error}")

            except Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
                continue
            except Exception as e:
                logger.error(f"Upload worker error: {e}")
                # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.1)

        logger.info("Memory upload worker stopped - Final memory usage: {self.current_memory_usage} bytes")

    def _convert_to_maven_path(self, artifact: MavenArtifact) -> str:
        """è½¬æ¢ä¸º Maven è·¯å¾„æ ¼å¼"""
        parts = artifact.file_path.split('/')
        if len(parts) >= 4:
            group_id = '.'.join(parts[:-3])
            artifact_id = parts[-3]
            version = parts[-2]
            filename = parts[-1]

            return f"{group_id.replace('.', '/')}/{artifact_id}/{version}/{filename}"

        return artifact.file_path

    def _generate_final_stats(self) -> None:
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        logger.info("=== Memory Pipeline Migration Statistics ===")
        logger.info(f"Total artifacts: {self.stats['total_artifacts']}")
        logger.info(f"Skipped (already uploaded): {self.stats['skipped_existing']}")
        logger.info(f"Downloaded: {self.stats['downloaded']}")
        logger.info(f"Download failed: {self.stats['download_failed']}")
        logger.info(f"Uploaded: {self.stats['uploaded']}")
        logger.info(f"Upload failed: {self.stats['upload_failed']}")

        if self.failed_tasks:
            logger.warning(f"Failed tasks: {len(self.failed_tasks)}")
            for task in self.failed_tasks[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                logger.warning(f"  - {task.artifact.file_path}: {task.error_message}")

        processed = self.stats['uploaded'] + self.stats['upload_failed']
        success_rate = (self.stats['uploaded'] / processed * 100) if processed > 0 else 0
        logger.info(f"Upload success rate: {success_rate:.1f}%")